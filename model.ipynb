{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":120777,"databundleVersionId":14442495,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":14073301,"sourceType":"datasetVersion","datasetId":8958389}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T07:14:51.834968Z","iopub.execute_input":"2025-12-08T07:14:51.835298Z","iopub.status.idle":"2025-12-08T07:14:58.363249Z","shell.execute_reply.started":"2025-12-08T07:14:51.835274Z","shell.execute_reply":"2025-12-08T07:14:58.362534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-08T07:14:58.364680Z","iopub.execute_input":"2025-12-08T07:14:58.364887Z","iopub.status.idle":"2025-12-08T07:14:58.626167Z","shell.execute_reply.started":"2025-12-08T07:14:58.364866Z","shell.execute_reply":"2025-12-08T07:14:58.625586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/df-ass4-small/train.csv\")[:1000]\nval_df =  pd.read_csv(\"/kaggle/input/df-ass4-small/val.csv\")[:500]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokinization","metadata":{}},{"cell_type":"code","source":"model_name = \"microsoft/mdeberta-v3-base\" # mDeBERTa-v3\n\nlabel_list = [\"O\", \"S-LOC\", \"I-LOC\"]\n\nid2label = {i: label for i, label in enumerate(label_list)}\nlabel2id = {label: i for i, label in enumerate(label_list)}\n\nbatch_size = 4\nepochs = 2\nlr = 3e-5\nnum_labels = len(label_list)\npadding = -100\nmax_len = 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T07:18:46.194183Z","iopub.execute_input":"2025-12-08T07:18:46.194668Z","iopub.status.idle":"2025-12-08T07:18:46.198995Z","shell.execute_reply.started":"2025-12-08T07:18:46.194642Z","shell.execute_reply":"2025-12-08T07:18:46.198297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_name) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T07:18:46.730238Z","iopub.execute_input":"2025-12-08T07:18:46.730957Z","iopub.status.idle":"2025-12-08T07:18:48.703771Z","shell.execute_reply.started":"2025-12-08T07:18:46.730932Z","shell.execute_reply":"2025-12-08T07:18:48.703153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\n\ndef char_markers_label(char_start, char_end, spans):\n        for (span_start, span_end) in spans:\n            if not (char_end <= span_start or char_start >= span_end):\n                if char_start == span_start:\n                    return \"S-LOC\"\n                else:\n                    return \"I-LOC\"\n        return None\n\n\n\ndef to_token_labels(text, loc_markers, tokenizer, label2id):\n    spans = []\n\n    if loc_markers is None:\n        loc_markers = []\n\n    if isinstance(loc_markers, str):\n        try:\n            loc_markers = ast.literal_eval(loc_markers)\n        except (SyntaxError, ValueError):\n            loc_markers = []\n\n    if not isinstance(loc_markers, (list, tuple)):\n        loc_markers = []\n\n    for span in loc_markers:\n        if not isinstance(span, (list, tuple)):\n            continue\n        if len(span) != 2:\n            continue\n        s, e = span\n        spans.append((int(s), int(e)))\n\n\n    encoded = tokenizer(\n        text,\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length=512,\n    )\n\n    offsets = encoded[\"offset_mapping\"]\n    labels = [label2id[\"O\"]] * len(offsets)\n\n    prev_was_loc = False\n    for i, (start, end) in enumerate(offsets):\n        if start == end:\n            labels[i] = label2id[\"O\"]\n            prev_was_loc = False\n            continue\n\n        lbl = char_markers_label(start, end, spans)\n        if lbl is None:\n            labels[i] = label2id[\"O\"]\n            prev_was_loc = False\n        else:\n            if lbl == \"I-LOC\" and not prev_was_loc:\n                lbl = \"S-LOC\"\n            labels[i] = label2id[lbl]\n            prev_was_loc = True\n\n    encoded[\"labels\"] = labels\n    \n    return encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T07:18:48.721448Z","iopub.execute_input":"2025-12-08T07:18:48.721666Z","iopub.status.idle":"2025-12-08T07:18:48.728933Z","shell.execute_reply.started":"2025-12-08T07:18:48.721649Z","shell.execute_reply":"2025-12-08T07:18:48.728144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_pandas(train_df[[\"text\", \"loc_markers\"]])\nval_dataset = Dataset.from_pandas(val_df[[\"text\", \"loc_markers\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T07:18:50.702293Z","iopub.execute_input":"2025-12-08T07:18:50.702824Z","iopub.status.idle":"2025-12-08T07:18:50.721924Z","shell.execute_reply.started":"2025-12-08T07:18:50.702798Z","shell.execute_reply":"2025-12-08T07:18:50.721408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode(df):\n    ttl = to_token_labels(df[\"text\"], df[\"loc_markers\"], tokenizer, label2id)\n    return ttl\n    \ntrain_dataset = train_dataset.map(encode, batched=False)\nval_dataset = val_dataset.map(encode, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T07:18:51.287152Z","iopub.execute_input":"2025-12-08T07:18:51.287834Z","iopub.status.idle":"2025-12-08T07:18:52.090417Z","shell.execute_reply.started":"2025-12-08T07:18:51.287810Z","shell.execute_reply":"2025-12-08T07:18:52.089647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = train_dataset.remove_columns('offset_mapping')\ntrain_dataset = train_dataset.remove_columns('loc_markers')\ntrain_dataset = train_dataset.remove_columns('__index_level_0__')\n\nval_dataset = val_dataset.remove_columns('offset_mapping')\nval_dataset = val_dataset.remove_columns('loc_markers')\nval_dataset = val_dataset.remove_columns('__index_level_0__')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T07:18:52.091499Z","iopub.execute_input":"2025-12-08T07:18:52.091801Z","iopub.status.idle":"2025-12-08T07:18:52.103569Z","shell.execute_reply.started":"2025-12-08T07:18:52.091782Z","shell.execute_reply":"2025-12-08T07:18:52.102867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    model_name,\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(\n    tokenizer=tokenizer,\n    label_pad_token_id=padding,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom seqeval.metrics import f1_score, precision_score, recall_score\n\ndef compute_metrics(eval_pred):\n    true_tags = []\n    pred_tags = []\n\n    id2label_local = id2label\n\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    \n    for p_seq, l_seq in zip(preds, labels):\n        t_seq = []\n        p_tags = []\n        \n        for p, l in zip(p_seq, l_seq):\n            if l == -100:\n                continue\n                \n            t_seq.append(id2label_local[l])\n            p_tags.append(id2label_local[p])\n            \n        true_tags.append(t_seq)\n        pred_tags.append(p_tags)\n\n    return {\n        \"precision\": precision_score(true_tags, pred_tags),\n        \"recall\": recall_score(true_tags, pred_tags),\n        \"f1\": f1_score(true_tags, pred_tags),\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./ner-mdeberta-v3\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=epochs,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=100,\n    report_to=\"none\",  \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/kse-ua-location-extraction-2025/test.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\ntexts = test[\"text\"].tolist()\n\nencoded = tokenizer(\n    texts,\n    padding=True,\n    truncation=True,\n    max_length=max_len,\n    return_offsets_mapping=True,\n)\n\ntest_dataset = Dataset.from_dict({\n    \"input_ids\": encoded[\"input_ids\"],\n    \"attention_mask\": encoded[\"attention_mask\"],\n})\n\npred_output = trainer.predict(test_dataset)\n\nlogits = pred_output.predictions \npred_ids = logits.argmax(-1)\n\noffsets = np.array(encoded[\"offset_mapping\"])\nattn = np.array(encoded[\"attention_mask\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(f\"{pred_output[:1]} /n\n# {logits[:1]}, \\n\n# {pred_ids[:1]}, \\n\n# {offsets[:1]}, \\n\n# {attn[:1]}, \\n\n# \")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_locations_strings = []\n\nfor i, text in enumerate(texts):\n    seq_ids = pred_ids[i]\n    seq_offsets = offsets[i]\n    seq_attn = attn[i]\n\n    spans = []\n    current_span = None\n\n    for token_id, (start, end), m in zip(seq_ids, seq_offsets, seq_attn):\n        if m == 0:\n            continue\n        if start == 0 and end == 0:\n            continue\n\n        label = id2label[int(token_id)]\n\n        if label.startswith(\"S-LOC\"):\n            if current_span is not None:\n                spans.append(current_span)\n            current_span = [start, end]\n        elif label.startswith(\"I-LOC\") and current_span is not None:\n            current_span[1] = end\n        else:\n            if current_span is not None:\n                spans.append(current_span)\n                current_span = None\n\n    if current_span is not None:\n        spans.append(current_span)\n\n    loc_strings = []\n    for (start, end) in spans:\n        loc = text[start:end].strip()\n        if loc:\n            loc_strings.append(loc)\n\n    unique_locs = list(dict.fromkeys(loc_strings))\n    locations_str = \", \".join(unique_locs)\n\n    all_locations_strings.append(locations_str)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = test[[\"text_id\", \"text\"]].copy()\npredictions[\"locations\"] = all_locations_strings\n\n# TODO: split locations\n\n# predictions.head()\npredictions[[\"text_id\", \"locations\"]].to_csv(\"baseline.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# Code Ends","metadata":{}}]}